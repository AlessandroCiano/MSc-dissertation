{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3a6dd867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New length of df: 30\n",
      "Weighted F1 Score: 0.6641\n",
      "Macro F1 Score: 0.6710\n",
      "F1 Scores by Class: {'behavior': np.float64(0.6666666666666665), 'indicator': np.float64(0.7142857142857143), 'ideation': np.float64(0.6363636363636365), 'attempt': np.float64(0.6666666666666666)}\n",
      "Class Distribution:\n",
      "  Class attempt: 2 examples (6.67%)\n",
      "  Class behavior: 10 examples (33.33%)\n",
      "  Class ideation: 12 examples (40.00%)\n",
      "  Class indicator: 6 examples (20.00%)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "############   Final ensemble  ############\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# load csv file\n",
    "df = pd.read_csv(r'F:\\VERO UTENTE\\Desktop\\Uni\\dissertation\\main\\K-fold\\large-cross-entropy--5e-5\\ensamble_explore\\TEST_alldata_ensemble_pred.csv')\n",
    "\n",
    "df_filter = pd.read_csv(r'F:\\VERO UTENTE\\Desktop\\Uni\\dissertation\\main\\data\\test_set_30reduced.csv')\n",
    "\n",
    "#get in df only the rows that are in df_filter\n",
    "df = df[df['index'].isin(df_filter['idx'])]\n",
    "\n",
    "#print new length of df\n",
    "print(f\"New length of df: {len(df)}\")\n",
    "\n",
    "#calculate weighted f1 score based on post_risk and predicted_label (4 classes)\n",
    "def show_metrics(df):\n",
    "    # Calculate the number of instances for each class\n",
    "    class_counts = df['post_risk'].value_counts(normalize=True)\n",
    "    class_counts_raw = df['post_risk'].value_counts()\n",
    "\n",
    "    # Calculate the F1 score for each class\n",
    "    f1_scores = {}\n",
    "    for label in df['post_risk'].unique():\n",
    "        true_positives = ((df['post_risk'] == label) & (df['predicted_label'] == label)).sum()\n",
    "        false_positives = ((df['post_risk'] != label) & (df['predicted_label'] == label)).sum()\n",
    "        false_negatives = ((df['post_risk'] == label) & (df['predicted_label'] != label)).sum()\n",
    "\n",
    "        precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0.0\n",
    "        recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0.0\n",
    "\n",
    "        if (precision + recall) > 0:\n",
    "            f1_scores[label] = 2 * (precision * recall) / (precision + recall)\n",
    "        else:\n",
    "            f1_scores[label] = 0.0\n",
    "\n",
    "    # Calculate the weighted F1 score\n",
    "    weighted_f1 = sum(class_counts[label] * f1_scores[label] for label in f1_scores)\n",
    "\n",
    "    # Calculate the macro F1 score (average of all class F1 scores)\n",
    "    macro_f1 = sum(f1_scores.values()) / len(f1_scores)\n",
    "\n",
    "    print(f\"Weighted F1 Score: {weighted_f1:.4f}\")\n",
    "    print(f\"Macro F1 Score: {macro_f1:.4f}\")\n",
    "    print(f\"F1 Scores by Class: {f1_scores}\")\n",
    "    \n",
    "    # Print both raw counts and proportions\n",
    "    print(\"Class Distribution:\")\n",
    "    for label in sorted(class_counts.index):\n",
    "        print(f\"  Class {label}: {class_counts_raw[label]} examples ({class_counts[label]:.2%})\")\n",
    "\n",
    "    return\n",
    "\n",
    "show_metrics(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_gpu_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
