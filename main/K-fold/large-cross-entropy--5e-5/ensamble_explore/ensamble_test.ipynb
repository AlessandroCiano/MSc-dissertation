{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a4a4b654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of common indices: 30\n",
      "First few common indices: [3, 4, 14, 15, 16]\n",
      "Combined predictions saved to F:\\VERO UTENTE\\Desktop\\Uni\\dissertation\\main\\K-fold\\large-cross-entropy--5e-5\\ensamble_explore\\TEST_alldata_ensemble_gemini+deep+FIRST.csv\n",
      "Number of filtered common samples: 30\n",
      "\n",
      "Sample predictions:\n",
      "Index 3: True=, Predicted=behavior\n",
      "Index 4: True=, Predicted=indicator\n",
      "Index 14: True=, Predicted=indicator\n",
      "Index 15: True=, Predicted=indicator\n",
      "Index 16: True=, Predicted=ideation\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "input_file_paths = [\n",
    "    r'F:\\VERO UTENTE\\Desktop\\Uni\\dissertation\\main\\K-fold\\large-cross-entropy--5e-5\\data_all\\FIRST\\test_results_alldata.csv',\n",
    "    r'F:\\VERO UTENTE\\Desktop\\Uni\\dissertation\\main\\NO-fold\\DeepSeek\\thinking\\test\\deepseek-reasoner_calculators_ev_gemini_1.1_results-temp=0.5.csv',\n",
    "    r'F:\\VERO UTENTE\\Desktop\\Uni\\dissertation\\main\\NO-fold\\Google\\non-thinking\\test\\gemini-2.5-flash-preview-05-20_calculators_results.csv',\n",
    "    #r'F:\\VERO UTENTE\\Desktop\\Uni\\dissertation\\main\\NO-fold\\Grok\\non-thinking\\test\\grok-3-mini-latest_calculators_results.csv',\n",
    "   # r'F:\\VERO UTENTE\\Desktop\\Uni\\dissertation\\main\\NO-fold\\Grok\\thinking\\test\\grok-3-mini-latest_calculators_results-temp=0.4.csv',\n",
    "]\n",
    "\n",
    "filter_file_path = r'F:\\VERO UTENTE\\Desktop\\Uni\\dissertation\\main\\data\\test_set_30reduced.csv'\n",
    "\n",
    "data_frames = [pd.read_csv(file_path) for file_path in input_file_paths]\n",
    "data_filter = pd.read_csv(filter_file_path)\n",
    "\n",
    "# Get the indices from the filter file\n",
    "filter_indices = set(data_filter['idx'])\n",
    "\n",
    "# Check which column is used for indexing across the dataframes\n",
    "# Find common indices across all dataframes that are also in the filter\n",
    "common_indices = set(data_frames[0]['index'])\n",
    "for df in data_frames[1:]:\n",
    "    common_indices.intersection_update(df['index'])\n",
    "\n",
    "# Filter using only indices that appear in data_filter\n",
    "common_indices = common_indices.intersection(filter_indices)\n",
    "\n",
    "print(f\"Number of common indices: {len(common_indices)}\")\n",
    "common_indices_list = sorted(list(common_indices))\n",
    "print(f\"First few common indices: {common_indices_list[:5]}\")\n",
    "\n",
    "CLASS_LABELS = [\"indicator\", \"ideation\", \"behavior\", \"attempt\"]\n",
    "# Adjust weights to use all models (example weights)\n",
    "#weights = [0.738, 0.750, 0.745, 0.774, 0.743]\n",
    "weights = [0.744, 0.750, 0.745]\n",
    "\n",
    "# Create new dataframe with common samples\n",
    "new_df = pd.DataFrame()\n",
    "new_df['index'] = common_indices_list\n",
    "\n",
    "# Get the original post and labels from the first dataframe\n",
    "for idx in common_indices_list:\n",
    "    mask = data_frames[0]['index'] == idx\n",
    "    if any(mask):\n",
    "        row_idx = mask.idxmax()\n",
    "        if 'post' not in new_df.columns:\n",
    "            new_df['post'] = \"\"\n",
    "            new_df['labels'] = \"\"\n",
    "        new_df.loc[new_df['index'] == idx, 'post'] = data_frames[0].loc[row_idx, 'post']\n",
    "        new_df.loc[new_df['index'] == idx, 'post_risk'] = data_frames[0].loc[row_idx, 'post_risk']\n",
    "\n",
    "# Create a weighted average of predictions\n",
    "weighted_sum = np.zeros((len(common_indices_list), len(CLASS_LABELS)))\n",
    "\n",
    "for i, df in enumerate(data_frames):\n",
    "    for j, idx in enumerate(common_indices_list):\n",
    "        mask = df['index'] == idx\n",
    "        if any(mask):\n",
    "            row_idx = mask.idxmax()\n",
    "            pred = df.loc[row_idx, 'predicted_label']\n",
    "            class_index = CLASS_LABELS.index(pred)\n",
    "            weighted_sum[j, class_index] += weights[i]\n",
    "\n",
    "# Determine final predictions\n",
    "final_predictions = []\n",
    "for i in range(len(common_indices_list)):\n",
    "    max_index = np.argmax(weighted_sum[i])\n",
    "    final_predictions.append(CLASS_LABELS[max_index])\n",
    "\n",
    "# Add final predictions to the dataframe\n",
    "new_df['predicted_label'] = final_predictions\n",
    "\n",
    "# Save the combined predictions\n",
    "output_path = r'F:\\VERO UTENTE\\Desktop\\Uni\\dissertation\\main\\K-fold\\large-cross-entropy--5e-5\\ensamble_explore\\TEST_alldata_ensemble_gemini+deep+FIRST.csv'\n",
    "new_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Combined predictions saved to {output_path}\")\n",
    "print(f\"Number of filtered common samples: {len(common_indices_list)}\")\n",
    "\n",
    "# Print a few example predictions\n",
    "print(\"\\nSample predictions:\")\n",
    "for i in range(min(5, len(new_df))):\n",
    "    print(f\"Index {new_df.iloc[i]['index']}: True={new_df.iloc[i]['labels']}, Predicted={new_df.iloc[i]['predicted_label']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "649c825e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:52: SyntaxWarning: invalid escape sequence '\\p'\n",
      "<>:52: SyntaxWarning: invalid escape sequence '\\p'\n",
      "C:\\Users\\Alessandro\\AppData\\Local\\Temp\\ipykernel_13028\\296895395.py:52: SyntaxWarning: invalid escape sequence '\\p'\n",
      "  print(f\"{metric}: {mean:.1f} \\pm {std:.1f}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['index', 'post', 'labels', 'post_risk', 'predicted_label'], dtype='object')\n",
      "\n",
      "Ensemble Model Performance (with standard deviation via bootstrapping):\n",
      "weighted_f1: 76.9 \\pm 7.7\n",
      "macro_f1: 80.1 \\pm 6.9\n",
      "accuracy: 77.0 \\pm 7.6\n"
     ]
    }
   ],
   "source": [
    "# First, fix the true labels extraction (add this after creating new_df but before the weighted average calculation)\n",
    "# Get the original post and labels from the first dataframe\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "new_df = pd.read_csv(r'F:\\VERO UTENTE\\Desktop\\Uni\\dissertation\\main\\K-fold\\large-cross-entropy--5e-5\\ensamble_explore\\TEST_alldata_ensemble_gemini+deep+FIRST.csv')\n",
    "#print new_def columns\n",
    "print(new_df.columns)\n",
    "\n",
    "for idx in common_indices_list:\n",
    "    mask = data_frames[0]['index'] == idx\n",
    "    if any(mask):\n",
    "        row_idx = mask.idxmax()\n",
    "        if 'post' not in new_df.columns:\n",
    "            new_df['post'] = \"\"\n",
    "            new_df['true_label_str'] = \"\"  # Initialize true_label_str column\n",
    "        new_df.loc[new_df['index'] == idx, 'post'] = data_frames[0].loc[row_idx, 'post']\n",
    "        if 'post_risk' in data_frames[0].columns:\n",
    "            new_df.loc[new_df['index'] == idx, 'post_risk'] = data_frames[0].loc[row_idx, 'post_risk']\n",
    "        if 'true_label_str' in data_frames[0].columns and 'post_risk' in data_frames[0].columns:\n",
    "            new_df.loc[new_df['index'] == idx, 'post_risk'] = data_frames[0].loc[row_idx, 'post_risk']\n",
    "        elif 'post_risk' in df.columns:\n",
    "            new_df.loc[new_df['index'] == idx, 'post_risk'] = df.loc[df['index'] == idx, 'post_risk'].values[0]\n",
    "        else:\n",
    "            new_df.loc[new_df['index'] == idx, 'post_risk'] = np.nan\n",
    "            new_df.loc[new_df['index'] == idx, 'post_risk'] = data_frames[0].loc[row_idx, 'post_risk']\n",
    "\n",
    "# Add this function and metrics calculation after the ensemble predictions\n",
    "def calculate_metrics_with_bootstrap(df, n_bootstrap=1000, random_state=42):\n",
    "    np.random.seed(random_state)\n",
    "    metrics = {'weighted_f1': [], 'macro_f1': [], 'accuracy': []}\n",
    "    for _ in range(n_bootstrap):\n",
    "        sample = df.sample(frac=1, replace=True)\n",
    "        metrics['weighted_f1'].append(f1_score(sample['post_risk'], sample['predicted_label'], average='weighted'))\n",
    "        metrics['macro_f1'].append(f1_score(sample['post_risk'], sample['predicted_label'], average='macro'))\n",
    "        metrics['accuracy'].append(accuracy_score(sample['post_risk'], sample['predicted_label']))\n",
    "    results = {\n",
    "        'weighted_f1': (np.mean(metrics['weighted_f1'])*100, np.std(metrics['weighted_f1'])*100),\n",
    "        'macro_f1': (np.mean(metrics['macro_f1'])*100, np.std(metrics['macro_f1'])*100),\n",
    "        'accuracy': (np.mean(metrics['accuracy'])*100, np.std(metrics['accuracy'])*100)\n",
    "    }\n",
    "    return results\n",
    "\n",
    "# Calculate metrics for the ensemble\n",
    "bootstrap_results = calculate_metrics_with_bootstrap(new_df)\n",
    "\n",
    "# Print the results\n",
    "print(\"\\nEnsemble Model Performance (with standard deviation via bootstrapping):\")\n",
    "for metric, (mean, std) in bootstrap_results.items():\n",
    "    print(f\"{metric}: {mean:.1f} \\pm {std:.1f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_gpu_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
