{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f90ee608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_label\n",
      "ideation     0.381588\n",
      "indicator    0.357572\n",
      "behavior     0.206805\n",
      "attempt      0.054036\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df1 = pd.read_csv('pseudol_output_alldata.csv')\n",
    "\n",
    "# Print class proportions\n",
    "proportions = df1['predicted_label'].value_counts(normalize=True)\n",
    "print(proportions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70b23eb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_label\n",
      "ideation     0.414276\n",
      "indicator    0.368245\n",
      "behavior     0.174116\n",
      "attempt      0.043362\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df2 = pd.read_csv('pseudol_output_fold1.csv')\n",
    "\n",
    "# Print class proportions\n",
    "proportions = df2['predicted_label'].value_counts(normalize=True)\n",
    "print(proportions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93c63230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples with the same predicted label in both datasets: 1285\n"
     ]
    }
   ],
   "source": [
    "# Check how many examples are predicted with the same label in both datasets\n",
    "same_label_count = (df1['predicted_label'] == df2['predicted_label']).sum()\n",
    "print(f\"Number of examples with the same predicted label in both datasets: {same_label_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4aec2b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N. of examples in df3: 871\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df3 = pd.read_csv('withoutL_predictions_comb_fold1+deepT+geminiN+grokT+grokN.csv')\n",
    "print(\"N. of examples in df3:\", len(df3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00a11dac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N. of examples in df_SECOND: 969\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_SECOND = r\"F:\\VERO UTENTE\\Desktop\\Uni\\dissertation\\main\\K-fold\\large-cross-entropy--5e-5\\data_fold1\\SECOND\\SECOND_pseudol_comb_fold1+deepT+geminiN+grokT+grokN.csv\"\n",
    "print(\"N. of examples in df_SECOND:\", len(pd.read_csv(df_SECOND)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5939e4",
   "metadata": {},
   "source": [
    "***COMBINING FIRST fold1 CODE BELOW***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c9e3aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N. of examples in df3: 1271\n",
      "labels\n",
      "ideation     0.394965\n",
      "indicator    0.358773\n",
      "behavior     0.187254\n",
      "attempt      0.059009\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df4 = pd.read_csv(r'F:\\VERO UTENTE\\Desktop\\Uni\\dissertation\\main\\K-fold\\large-cross-entropy--5e-5\\data_fold1\\FIRST_combined_train_pseudol.csv')\n",
    "print(\"N. of examples in df3:\", len(df4))\n",
    "\n",
    "# Print class proportions\n",
    "proportions = df4['labels'].value_counts(normalize=True)\n",
    "print(proportions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "268b03f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400 871\n",
      "Number of examples in half_pseudol_df: 436\n",
      "     index                                               post     labels\n",
      "0        0  Never will find love. Kind of thinking about e...   behavior\n",
      "1        1            I'm inconsolable. I'm going to die soon   ideation\n",
      "2        2  I hate living. sometime i think about why i ha...   ideation\n",
      "3        3  Help me kill myself. Please somebody help me f...   behavior\n",
      "4        4  Is there a painless way to die. I'm struggling...   ideation\n",
      "..     ...                                                ...        ...\n",
      "831    500  Why can’t I function in the world at all? I’m ...   ideation\n",
      "832   1981  It feels so bad to know that there is no possi...  indicator\n",
      "833   1264                    I can't take this shit anymore   indicator\n",
      "834   1785  No one seems to understand me So my main probl...   ideation\n",
      "835   1167  I guess my body started trashing Tried low han...    attempt\n",
      "\n",
      "[836 rows x 3 columns]\n",
      "Class proportions in combined dataset:\n",
      "labels\n",
      "ideation     0.394737\n",
      "indicator    0.337321\n",
      "behavior     0.205742\n",
      "attempt      0.062201\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "train_df = pd.read_csv('data_fold1/fold_1_train_split.csv')\n",
    "pseudol_df = pd.read_csv('data_fold1/withoutL_predictions_comb_fold1+deepT+geminiN+grokT+grokN.csv')\n",
    "print(len(train_df), len(pseudol_df))\n",
    "\n",
    "# Take only half of the examples from pseudol_df\n",
    "half_pseudol_df = pseudol_df.sample(frac=0.5, random_state=42)\n",
    "print(f\"Number of examples in half_pseudol_df: {len(half_pseudol_df)}\")\n",
    "\n",
    "# Rename the column in half_pseudol_df\n",
    "half_pseudol_df = half_pseudol_df.rename(columns={'predicted_label': 'labels'})\n",
    "\n",
    "# Combine train_df with half of pseudol_df\n",
    "combined_df = pd.concat([train_df[['index', 'post', 'labels']], half_pseudol_df[['index', 'post', 'labels']]], ignore_index=True)\n",
    "print(combined_df)\n",
    "\n",
    "# Print class proportions in the combined dataset\n",
    "proportions = combined_df['labels'].value_counts(normalize=True)\n",
    "print(\"Class proportions in combined dataset:\")\n",
    "print(proportions)\n",
    "\n",
    "# Save the combined dataframe to a new CSV file\n",
    "combined_df.to_csv('FIRST_HALF_combined_train_pseudol.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b0633eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N. of examples in df3: 836\n",
      "labels\n",
      "ideation     0.394737\n",
      "indicator    0.337321\n",
      "behavior     0.205742\n",
      "attempt      0.062201\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df4 = pd.read_csv(r'F:\\VERO UTENTE\\Desktop\\Uni\\dissertation\\main\\K-fold\\large-cross-entropy--5e-5\\data_fold1\\FIRST_HALF_combined_train_pseudol.csv')\n",
    "print(\"N. of examples in df3:\", len(df4))\n",
    "\n",
    "# Print class proportions\n",
    "proportions = df4['labels'].value_counts(normalize=True)\n",
    "print(proportions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68cabdcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_df = pd.read_csv('data_fold1/fold_1_train_split.csv')\n",
    "pseudol_df = pd.read_csv('data_fold1/withoutL_predictions_comb_fold1+deepT+geminiN+grokT+grokN.csv')\n",
    "\n",
    "# Take only half of the examples from pseudol_df\n",
    "oneforth_pseudol_df = pseudol_df.sample(frac=0.25, random_state=42)\n",
    "print(f\"Number of examples in half_pseudol_df: {len(oneforth_pseudol_df)}\")\n",
    "print(len(train_df), len(oneforth_pseudol_df))\n",
    "\n",
    "# Rename the column in half_pseudol_df\n",
    "oneforth_pseudol_df = oneforth_pseudol_df.rename(columns={'predicted_label': 'labels'})\n",
    "\n",
    "# Combine train_df with half of pseudol_df\n",
    "combined_df = pd.concat([train_df[['index', 'post', 'labels']], oneforth_pseudol_df[['index', 'post', 'labels']]], ignore_index=True)\n",
    "print(combined_df)\n",
    "\n",
    "# Print class proportions in the combined dataset\n",
    "proportions = combined_df['labels'].value_counts(normalize=True)\n",
    "print(\"Class proportions in combined dataset:\")\n",
    "print(proportions)\n",
    "\n",
    "# Save the combined dataframe to a new CSV file\n",
    "combined_df.to_csv('data_fold1/FIRST_ONEFORTH_combined_train_pseudol.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a99b170",
   "metadata": {},
   "source": [
    "***COMBINING SECOND fold1 CODE BELOW***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7a6d258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400 969\n",
      "      index                                               post     labels\n",
      "0         0  Never will find love. Kind of thinking about e...   behavior\n",
      "1         1            I'm inconsolable. I'm going to die soon   ideation\n",
      "2         2  I hate living. sometime i think about why i ha...   ideation\n",
      "3         3  Help me kill myself. Please somebody help me f...   behavior\n",
      "4         4  Is there a painless way to die. I'm struggling...   ideation\n",
      "...     ...                                                ...        ...\n",
      "1364   1992  I am in pain today. I keep trying to move on w...    attempt\n",
      "1365   1993  took 10,000 mg aspirin do i have to go to the ...    attempt\n",
      "1366   1994  I hate myself I cheated on my boyfriend. I mis...   behavior\n",
      "1367   1996            Why was I even born I just want to die.   ideation\n",
      "1368   1997  Reached a place I haven't been at in a while a...  indicator\n",
      "\n",
      "[1369 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv(r'F:\\VERO UTENTE\\Desktop\\Uni\\dissertation\\main\\K-fold\\large-cross-entropy--5e-5\\data_fold1\\fold_1_train_split.csv')\n",
    "pseudol_df = pd.read_csv(r'F:\\VERO UTENTE\\Desktop\\Uni\\dissertation\\main\\K-fold\\large-cross-entropy--5e-5\\data_fold1\\SECOND\\SECOND_pseudol_comb_fold1+deepT+geminiN+grokT+grokN.csv')\n",
    "print(len(train_df), len(pseudol_df))\n",
    "\n",
    "# combine the two dataframes, considering only the columns 'index', 'post', 'predicted_label'\n",
    "train_df = train_df.rename(columns={'post_risk': 'labels'})\n",
    "pseudol_df = pseudol_df.rename(columns={'predicted_label': 'labels'})\n",
    "\n",
    "combined_df = pd.concat([train_df[['index', 'post', 'labels']], pseudol_df[['index', 'post', 'labels']]], ignore_index=True)\n",
    "print(combined_df)\n",
    "\n",
    "# Save the combined dataframe to a new CSV file\n",
    "combined_df.to_csv(r'F:\\VERO UTENTE\\Desktop\\Uni\\dissertation\\main\\K-fold\\large-cross-entropy--5e-5\\data_fold1\\SECOND\\SECOND_combined_train_pseudol.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0607a8da",
   "metadata": {},
   "source": [
    "***COMBINING alldata CODE BELOW***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70975e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N. of examples in df3: 889\n",
      "predicted_label\n",
      "indicator    0.392576\n",
      "ideation     0.373453\n",
      "behavior     0.174353\n",
      "attempt      0.059618\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df4 = pd.read_csv(r'F:\\VERO UTENTE\\Desktop\\Uni\\dissertation\\main\\K-fold\\large-cross-entropy--5e-5\\data_all\\withoutL_predictions_comb_alldata+deepT+geminiN+grokT+grokN.csv')\n",
    "print(\"N. of examples in df3:\", len(df4))\n",
    "\n",
    "# Print class proportions\n",
    "proportions = df4['predicted_label'].value_counts(normalize=True)\n",
    "print(proportions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb9a9947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 889\n",
      "      index                                               post     labels\n",
      "0         0  Never will find love. Kind of thinking about e...   behavior\n",
      "1         1            I'm inconsolable. I'm going to die soon   ideation\n",
      "2         2  I hate living. sometime i think about why i ha...   ideation\n",
      "3         3  Help me kill myself. Please somebody help me f...   behavior\n",
      "4         4  Is there a painless way to die. I'm struggling...   ideation\n",
      "...     ...                                                ...        ...\n",
      "1384   1991  I have attempted suicide once but it didn't wo...    attempt\n",
      "1385   1992  I am in pain today. I keep trying to move on w...    attempt\n",
      "1386   1994  I hate myself I cheated on my boyfriend. I mis...   behavior\n",
      "1387   1996            Why was I even born I just want to die.   ideation\n",
      "1388   1997  Reached a place I haven't been at in a while a...  indicator\n",
      "\n",
      "[1389 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv(r'F:\\VERO UTENTE\\Desktop\\Uni\\dissertation\\main\\data\\posts_with_labels.csv')\n",
    "pseudol_df = pd.read_csv(r'F:\\VERO UTENTE\\Desktop\\Uni\\dissertation\\main\\K-fold\\large-cross-entropy--5e-5\\data_all\\withoutL_predictions_comb_alldata+deepT+geminiN+grokT+grokN.csv')\n",
    "print(len(train_df), len(pseudol_df))\n",
    "\n",
    "# combine the two dataframes, considering only the columns 'index', 'post', 'predicted_label'\n",
    "train_df = train_df.rename(columns={'post_risk': 'labels'})\n",
    "pseudol_df = pseudol_df.rename(columns={'predicted_label': 'labels'})\n",
    "\n",
    "combined_df = pd.concat([train_df[['index', 'post', 'labels']], pseudol_df[['index', 'post', 'labels']]], ignore_index=True)\n",
    "print(combined_df)\n",
    "\n",
    "# Save the combined dataframe to a new CSV file\n",
    "combined_df.to_csv(r'F:\\VERO UTENTE\\Desktop\\Uni\\dissertation\\main\\K-fold\\large-cross-entropy--5e-5\\data_all\\FIRST_combined_train_pseudol.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d43fc68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N. of examples in FIRST_pesudol_comb_fold1+deepT+geminiN+grokT+grokN.csv: 871\n",
      "N. of examples in FIRST_pseudol_comb_fold2+deepT+geminiN+grokT+grokN.csv: 891\n",
      "N. of examples in FIRST_pseudol_comb_fold3+deepT+geminiN+grokT+grokN.csv: 868\n",
      "N. of examples in FIRST_pseudol_comb_fold4+deepT+geminiN+grokT+grokN.csv: 890\n",
      "N. of examples in FIRST_pseudol_comb_fold5+deepT+geminiN+grokT+grokN.csv: 905\n",
      "Average number of examples across all files: 885.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "path = [\n",
    "    r'F:\\VERO UTENTE\\Desktop\\Uni\\dissertation\\main\\K-fold\\large-cross-entropy--5e-5\\data_fold1\\FIRST\\FIRST_pesudol_comb_fold1+deepT+geminiN+grokT+grokN.csv',\n",
    "    r'F:\\VERO UTENTE\\Desktop\\Uni\\dissertation\\main\\K-fold\\large-cross-entropy--5e-5\\data_fold2\\FIRST\\FIRST_pseudol_comb_fold2+deepT+geminiN+grokT+grokN.csv',\n",
    "    r'F:\\VERO UTENTE\\Desktop\\Uni\\dissertation\\main\\K-fold\\large-cross-entropy--5e-5\\data_fold3\\FIRST\\FIRST_pseudol_comb_fold3+deepT+geminiN+grokT+grokN.csv',\n",
    "    r'F:\\VERO UTENTE\\Desktop\\Uni\\dissertation\\main\\K-fold\\large-cross-entropy--5e-5\\data_fold4\\FIRST\\FIRST_pseudol_comb_fold4+deepT+geminiN+grokT+grokN.csv',\n",
    "    r'F:\\VERO UTENTE\\Desktop\\Uni\\dissertation\\main\\K-fold\\large-cross-entropy--5e-5\\data_fold5\\FIRST\\FIRST_pseudol_comb_fold5+deepT+geminiN+grokT+grokN.csv'\n",
    "]\n",
    "\n",
    "total = 0\n",
    "for p in path:\n",
    "    df = pd.read_csv(p)\n",
    "    total += len(df)\n",
    "    print(f\"N. of examples in {os.path.basename(p)}:\", len(df))\n",
    "\n",
    "avg = total / len(path)\n",
    "print(f\"Average number of examples across all files: {avg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a87d74e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400 1055\n",
      "      index                                               post     labels\n",
      "0         0  Never will find love. Kind of thinking about e...   behavior\n",
      "1         2  I hate living. sometime i think about why i ha...   ideation\n",
      "2         3  Help me kill myself. Please somebody help me f...   behavior\n",
      "3         4  Is there a painless way to die. I'm struggling...   ideation\n",
      "4         5  whats the point of living. no really. is there...   ideation\n",
      "...     ...                                                ...        ...\n",
      "1450   1992  I am in pain today. I keep trying to move on w...    attempt\n",
      "1451   1993  took 10,000 mg aspirin do i have to go to the ...    attempt\n",
      "1452   1994  I hate myself I cheated on my boyfriend. I mis...   behavior\n",
      "1453   1996            Why was I even born I just want to die.   ideation\n",
      "1454   1997  Reached a place I haven't been at in a while a...  indicator\n",
      "\n",
      "[1455 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "n = 5\n",
    "fold = 'fold' + str(n)\n",
    "order = 'FIRST'\n",
    "\n",
    "train_df = pd.read_csv(os.path.join(r'F:\\VERO UTENTE\\Desktop\\Uni\\dissertation\\main\\K-fold\\large-cross-entropy--5e-5', 'data_' + fold, 'fold_' + str(n) + '_train_split.csv'))\n",
    "#pseudol_df = pd.read_csv(os.path.join(r'F:\\VERO UTENTE\\Desktop\\Uni\\dissertation\\main\\K-fold\\large-cross-entropy--5e-5', 'data_' + fold, order, order + f'_pseudol_comb_' + fold+ '+deepT+geminiN+grokT+grokN.csv'))\n",
    "pseudol_df = pd.read_csv(r'F:\\VERO UTENTE\\Desktop\\Uni\\dissertation\\main\\NO-fold\\pseudol_comb_deepT+geminiN+grokT+grokN.csv')\n",
    "\n",
    "print(len(train_df), len(pseudol_df))\n",
    "\n",
    "# combine the two dataframes, considering only the columns 'index', 'post', 'predicted_label'\n",
    "train_df = train_df.rename(columns={'post_risk': 'labels'})\n",
    "pseudol_df = pseudol_df.rename(columns={'predicted_label': 'labels'})\n",
    "\n",
    "combined_df = pd.concat([train_df[['index', 'post', 'labels']], pseudol_df[['index', 'post', 'labels']]], ignore_index=True)\n",
    "print(combined_df)\n",
    "\n",
    "# Save the combined dataframe to a new CSV file\n",
    "#combined_df.to_csv(os.path.join(r'F:\\VERO UTENTE\\Desktop\\Uni\\dissertation\\main\\K-fold\\large-cross-entropy--5e-5', 'data_' + fold, order, order + '_combined_train_pseudol.csv'), index=False)\n",
    "combined_df.to_csv(os.path.join(r'F:\\VERO UTENTE\\Desktop\\Uni\\dissertation\\main\\K-fold\\large-cross-entropy--5e-5', 'data_' + fold, order + '_only_llm', 'combinedLLM_train_pseudol.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ad65bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Differences in N.:\n",
      "Fold 1: 98 examples\n",
      "Fold 2: 66 examples\n",
      "Fold 3: 97 examples\n",
      "Fold 4: 80 examples\n",
      "Fold 5: 72 examples\n",
      "Avg Difference: 82.60 examples\n",
      "Avg N. of examples in FIRST: 1285.0\n",
      "Avg N. of examples in SECOND: 1367.6\n",
      "N. of examples in only LLM paths: 1455\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "first_files = [\n",
    "    r'F:\\VERO UTENTE\\Desktop\\Uni\\dissertation\\main\\K-fold\\large-cross-entropy--5e-5\\data_fold1\\FIRST\\FIRST_combined_train_pseudol.csv',\n",
    "    r'F:\\VERO UTENTE\\Desktop\\Uni\\dissertation\\main\\K-fold\\large-cross-entropy--5e-5\\data_fold2\\FIRST\\FIRST_combined_train_pseudol.csv',\n",
    "    r'F:\\VERO UTENTE\\Desktop\\Uni\\dissertation\\main\\K-fold\\large-cross-entropy--5e-5\\data_fold3\\FIRST\\FIRST_combined_train_pseudol.csv',\n",
    "    r'F:\\VERO UTENTE\\Desktop\\Uni\\dissertation\\main\\K-fold\\large-cross-entropy--5e-5\\data_fold4\\FIRST\\FIRST_combined_train_pseudol.csv',\n",
    "    r'F:\\VERO UTENTE\\Desktop\\Uni\\dissertation\\main\\K-fold\\large-cross-entropy--5e-5\\data_fold5\\FIRST\\FIRST_combined_train_pseudol.csv'\n",
    "]\n",
    "\n",
    "second_files = [\n",
    "    r'F:\\VERO UTENTE\\Desktop\\Uni\\dissertation\\main\\K-fold\\large-cross-entropy--5e-5\\data_fold1\\SECOND\\SECOND_combined_train_pseudol.csv',\n",
    "    r'F:\\VERO UTENTE\\Desktop\\Uni\\dissertation\\main\\K-fold\\large-cross-entropy--5e-5\\data_fold2\\SECOND\\SECOND_combined_train_pseudol.csv',\n",
    "    r'F:\\VERO UTENTE\\Desktop\\Uni\\dissertation\\main\\K-fold\\large-cross-entropy--5e-5\\data_fold3\\SECOND\\SECOND_combined_train_pseudol.csv',\n",
    "    r'F:\\VERO UTENTE\\Desktop\\Uni\\dissertation\\main\\K-fold\\large-cross-entropy--5e-5\\data_fold4\\SECOND\\SECOND_combined_train_pseudol.csv',\n",
    "    r'F:\\VERO UTENTE\\Desktop\\Uni\\dissertation\\main\\K-fold\\large-cross-entropy--5e-5\\data_fold5\\SECOND\\SECOND_combined_train_pseudol.csv'\n",
    "]\n",
    "\n",
    "only_llm_paths = [\n",
    "    r\"F:\\VERO UTENTE\\Desktop\\Uni\\dissertation\\main\\K-fold\\large-cross-entropy--5e-5\\data_fold1\\FIRST_only_llm\\combinedLLM_train_pseudol.csv\",\n",
    "    r\"F:\\VERO UTENTE\\Desktop\\Uni\\dissertation\\main\\K-fold\\large-cross-entropy--5e-5\\data_fold2\\FIRST_only_llm\\combinedLLM_train_pseudol.csv\",\n",
    "    r\"F:\\VERO UTENTE\\Desktop\\Uni\\dissertation\\main\\K-fold\\large-cross-entropy--5e-5\\data_fold3\\FIRST_only_llm\\combinedLLM_train_pseudol.csv\",\n",
    "    r\"F:\\VERO UTENTE\\Desktop\\Uni\\dissertation\\main\\K-fold\\large-cross-entropy--5e-5\\data_fold4\\FIRST_only_llm\\combinedLLM_train_pseudol.csv\",\n",
    "    r\"F:\\VERO UTENTE\\Desktop\\Uni\\dissertation\\main\\K-fold\\large-cross-entropy--5e-5\\data_fold5\\FIRST_only_llm\\combinedLLM_train_pseudol.csv\"\n",
    "]\n",
    "\n",
    "differences = []\n",
    "avg_first = 0\n",
    "avg_second = 0\n",
    "n_only_llm = len(pd.read_csv(only_llm_paths[3]))\n",
    "for i in range(len(first_files)):\n",
    "    df1 = pd.read_csv(first_files[i])\n",
    "    df2 = pd.read_csv(second_files[i])\n",
    "    diff = len(df2) - len(df1)\n",
    "    differences.append(diff)\n",
    "    avg_first += len(df1)\n",
    "    avg_second += len(df2)\n",
    "\n",
    "avg_first /= len(first_files)\n",
    "avg_second /= len(second_files)\n",
    "\n",
    "print(\"Differences in N.:\")\n",
    "for i, diff in enumerate(differences):\n",
    "    print(f\"Fold {i+1}: {diff} examples\")\n",
    "\n",
    "print(f\"Avg Difference: {avg_second - avg_first:.2f} examples\")\n",
    "print(f\"Avg N. of examples in FIRST: {avg_first}\")\n",
    "print(f\"Avg N. of examples in SECOND: {avg_second}\")\n",
    "print(\"N. of examples in only LLM paths:\", n_only_llm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_gpu_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
