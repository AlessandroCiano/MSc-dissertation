{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5dc73ed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for the full DataFrame:\n",
      "\n",
      "Number of examples: 100\n",
      "Proportion and number of examples per class:\n",
      "ideation: 40.00% (40)\n",
      "behavior: 27.00% (27)\n",
      "indicator: 26.00% (26)\n",
      "attempt: 7.00% (7)\n",
      "------------------------------\n",
      "\n",
      "Metrics for filtered data from: f:\\VERO UTENTE\\Desktop\\Uni\\dissertation\\main\\K-fold\\large-cross-entropy--5e-5\\data_splits\\fold1_validation_split_predictions.csv\n",
      "\n",
      "Accuracy: 0.7100\n",
      "Weighted F1 Score: 0.7026\n",
      "Macro F1 Score: 0.6892\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     attempt       0.55      0.86      0.67         7\n",
      "    behavior       0.77      0.74      0.75        27\n",
      "    ideation       0.69      0.82      0.75        40\n",
      "   indicator       0.80      0.46      0.59        26\n",
      "\n",
      "    accuracy                           0.71       100\n",
      "   macro avg       0.70      0.72      0.69       100\n",
      "weighted avg       0.73      0.71      0.70       100\n",
      "\n",
      "\n",
      "Threshold used for filtering: 0.999\n",
      "\n",
      "Metrics for the filtered DataFrame:\n",
      "\n",
      "Number of examples: 25\n",
      "Proportion and number of examples per class:\n",
      "ideation: 52.00% (13)\n",
      "behavior: 24.00% (6)\n",
      "indicator: 20.00% (5)\n",
      "attempt: 4.00% (1)\n",
      "------------------------------\n",
      "\n",
      "Metrics for filtered data from: f:\\VERO UTENTE\\Desktop\\Uni\\dissertation\\main\\K-fold\\large-cross-entropy--5e-5\\data_splits\\fold1_validation_split_predictions.csv\n",
      "\n",
      "Accuracy: 0.7200\n",
      "Weighted F1 Score: 0.6426\n",
      "Macro F1 Score: 0.6519\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     attempt       1.00      1.00      1.00         1\n",
      "    behavior       0.83      0.83      0.83         6\n",
      "    ideation       0.67      0.92      0.77        13\n",
      "   indicator       0.00      0.00      0.00         5\n",
      "\n",
      "    accuracy                           0.72        25\n",
      "   macro avg       0.62      0.69      0.65        25\n",
      "weighted avg       0.59      0.72      0.64        25\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "# Define the path to your CSV file\n",
    "# Using a raw string (r'...') is a good practice for file paths on Windows\n",
    "file_path = r'f:\\VERO UTENTE\\Desktop\\Uni\\dissertation\\main\\K-fold\\large-cross-entropy--5e-5\\data_splits\\fold1_validation_split_predictions.csv'\n",
    "\n",
    "def print_metrics(data_frame):\n",
    "    print(f\"\\nNumber of examples: {len(data_frame)}\")\n",
    "    \n",
    "    if not data_frame.empty:\n",
    "        print(\"Proportion and number of examples per class:\")\n",
    "        \n",
    "        # Get value counts and proportions\n",
    "        counts = data_frame['labels'].value_counts()\n",
    "        proportions = data_frame['labels'].value_counts(normalize=True)\n",
    "        \n",
    "        # Combine them for a prettier output\n",
    "        for label, proportion in proportions.items():\n",
    "            count = counts[label]\n",
    "            print(f\"{label}: {proportion:.2%} ({count})\")\n",
    "        print(\"-\" * 30)\n",
    "\n",
    "        # Extract the true labels and predicted labels from the DataFrame\n",
    "        y_true = data_frame['labels']\n",
    "        y_pred = data_frame['predicted_label']\n",
    "        \n",
    "        # Get the unique labels present in the data\n",
    "        labels = sorted(pd.concat([y_true, y_pred]).unique())\n",
    "\n",
    "        # Calculate accuracy\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "        # Calculate weighted F1 score\n",
    "        weighted_f1 = f1_score(y_true, y_pred, average='weighted', labels=labels, zero_division=0)\n",
    "\n",
    "        # Calculate macro F1 score\n",
    "        macro_f1 = f1_score(y_true, y_pred, average='macro', labels=labels, zero_division=0)\n",
    "\n",
    "        # Generate a detailed classification report\n",
    "        report = classification_report(y_true, y_pred, labels=labels, zero_division=0)\n",
    "\n",
    "        # Print the calculated metrics\n",
    "        print(f\"\\nMetrics for filtered data from: {file_path}\\n\")\n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"Weighted F1 Score: {weighted_f1:.4f}\")\n",
    "        print(f\"Macro F1 Score: {macro_f1:.4f}\")\n",
    "        print(report)\n",
    "    else:\n",
    "        print(\"\\nNo examples in data_frame.\")\n",
    "\n",
    "def threshold_filter(data_frame, threshold):\n",
    "    # Define the probability columns\n",
    "    prob_cols = ['prob_indicator', 'prob_ideation', 'prob_behavior', 'prob_attempt']\n",
    "    \n",
    "    df = data_frame.copy()\n",
    "\n",
    "    # Check if all probability columns exist in the DataFrame\n",
    "    if not all(col in df.columns for col in prob_cols):\n",
    "        raise KeyError(f\"One or more probability columns are missing. Required: {prob_cols}\")\n",
    "\n",
    "    # Calculate the maximum probability for each row\n",
    "    df['max_prob'] = df[prob_cols].max(axis=1)\n",
    "\n",
    "    # Filter the DataFrame based on the max probability threshold\n",
    "    filtered_df = df[df['max_prob'] >= threshold].copy()\n",
    "\n",
    "    return filtered_df\n",
    "\n",
    "# Load the CSV file into a pandas DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "#print full DataFrame metrics\n",
    "print(\"Metrics for the full DataFrame:\")\n",
    "print_metrics(df)\n",
    "\n",
    "# Apply the threshold filter to the DataFrame\n",
    "threshold =0.999\n",
    "filtered_df = threshold_filter(df, threshold=threshold)\n",
    "\n",
    "# Print metrics for the filtered DataFrame\n",
    "print(f\"\\nThreshold used for filtering: {threshold}\")\n",
    "print(\"\\nMetrics for the filtered DataFrame:\")\n",
    "print_metrics(filtered_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8428c02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "# Define the path to your CSV file\n",
    "# Using a raw string (r'...') is a good practice for file paths on Windows\n",
    "file_path = r'f:\\VERO UTENTE\\Desktop\\Uni\\dissertation\\main\\K-fold\\large-cross-entropy--5e-5\\data_splits\\fold1_validation_split_predictions.csv'\n",
    "\n",
    "def print_metrics(data_frame):\n",
    "    print(f\"\\nNumber of examples: {len(data_frame)}\")\n",
    "    \n",
    "    if not data_frame.empty:\n",
    "        print(\"Proportion and number of examples per class:\")\n",
    "        \n",
    "        # Get value counts and proportions\n",
    "        counts = data_frame['labels'].value_counts()\n",
    "        proportions = data_frame['labels'].value_counts(normalize=True)\n",
    "        \n",
    "        # Combine them for a prettier output\n",
    "        for label, proportion in proportions.items():\n",
    "            count = counts[label]\n",
    "            print(f\"{label}: {proportion:.2%} ({count})\")\n",
    "        print(\"-\" * 30)\n",
    "\n",
    "        # Extract the true labels and predicted labels from the DataFrame\n",
    "        y_true = data_frame['labels']\n",
    "        y_pred = data_frame['predicted_label']\n",
    "        \n",
    "        # Get the unique labels present in the data\n",
    "        labels = sorted(pd.concat([y_true, y_pred]).unique())\n",
    "\n",
    "        # Calculate accuracy\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "        # Calculate weighted F1 score\n",
    "        weighted_f1 = f1_score(y_true, y_pred, average='weighted', labels=labels, zero_division=0)\n",
    "\n",
    "        # Calculate macro F1 score\n",
    "        macro_f1 = f1_score(y_true, y_pred, average='macro', labels=labels, zero_division=0)\n",
    "\n",
    "        # Generate a detailed classification report\n",
    "        report = classification_report(y_true, y_pred, labels=labels, zero_division=0)\n",
    "\n",
    "        # Print the calculated metrics\n",
    "        print(f\"\\nMetrics for filtered data from: {file_path}\\n\")\n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"Weighted F1 Score: {weighted_f1:.4f}\")\n",
    "        print(f\"Macro F1 Score: {macro_f1:.4f}\")\n",
    "        print(report)\n",
    "    else:\n",
    "        print(\"\\nNo examples in data_frame.\")\n",
    "\n",
    "def threshold_filter(data_frame, threshold):\n",
    "    # Define the probability columns\n",
    "    prob_cols = ['prob_indicator', 'prob_ideation', 'prob_behavior', 'prob_attempt']\n",
    "    \n",
    "    df = data_frame.copy()\n",
    "\n",
    "    # Check if all probability columns exist in the DataFrame\n",
    "    if not all(col in df.columns for col in prob_cols):\n",
    "        raise KeyError(f\"One or more probability columns are missing. Required: {prob_cols}\")\n",
    "\n",
    "    # Calculate the maximum probability for each row\n",
    "    df['max_prob'] = df[prob_cols].max(axis=1)\n",
    "\n",
    "    # Filter the DataFrame based on the max probability threshold\n",
    "    filtered_df = df[df['max_prob'] >= threshold].copy()\n",
    "\n",
    "    return filtered_df\n",
    "\n",
    "# Load the CSV file into a pandas DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "#print full DataFrame metrics\n",
    "print(\"Metrics for the full DataFrame:\")\n",
    "print_metrics(df)\n",
    "\n",
    "# Apply the threshold filter to the DataFrame\n",
    "threshold =0.999\n",
    "filtered_df = threshold_filter(df, threshold=threshold)\n",
    "\n",
    "# Print metrics for the filtered DataFrame\n",
    "print(f\"\\nThreshold used for filtering: {threshold}\")\n",
    "print(\"\\nMetrics for the filtered DataFrame:\")\n",
    "print_metrics(filtered_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_gpu_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
