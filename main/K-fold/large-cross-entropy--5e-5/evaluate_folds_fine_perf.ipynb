{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b35f705b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for original:\n",
      "Range of epochs considered: 10 < epoch < 90\n",
      "Mean weighted F1 score: 0.6634883889628587\n",
      "Standard deviation of weighted F1 score: 0.022994020023382675\n",
      "\n",
      "\n",
      "Metrics for warmup320:\n",
      "Range of epochs considered: 10 < epoch < 90\n",
      "Mean weighted F1 score: 0.6961561861142411\n",
      "Standard deviation of weighted F1 score: 0.015151783964880181\n",
      "\n",
      "\n",
      "Metrics for warmup100:\n",
      "Range of epochs considered: 10 < epoch < 90\n",
      "Mean weighted F1 score: 0.6910241143147076\n",
      "Standard deviation of weighted F1 score: 0.018012510164606525\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path_dict = {\n",
    "    \"original\": r\"F:\\VERO UTENTE\\Desktop\\Uni\\dissertation\\main\\K-fold\\large-cross-entropy--5e-5\\metrics\\roberta-large_metrics_fold_1.csv\",\n",
    "    \"warmup320\": r\"F:\\VERO UTENTE\\Desktop\\Uni\\dissertation\\main\\K-fold\\large-cross-entropy--5e-5\\fold1_FIRST_finetune\\warmup320\\metrics\\roberta-large_metrics.csv\",\n",
    "    \"warmup100\": r\"F:\\VERO UTENTE\\Desktop\\Uni\\dissertation\\main\\K-fold\\large-cross-entropy--5e-5\\fold1_FIRST_finetune\\warmup100\\metrics\\roberta-large_metrics.csv\"\n",
    "}\n",
    "\n",
    "def load_and_print_metrics(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    # average adn std of weighted f1 score (epoch > 10 AND epoch < 20)\n",
    "    avg_f1 = df.loc[(df['epoch'] > 10) & (df['epoch'] < 90), 'eval_f1_weighted'].mean()\n",
    "    std_f1 = df.loc[(df['epoch'] > 10) & (df['epoch'] < 90), 'eval_f1_weighted'].std()\n",
    "\n",
    "    print(\"Range of epochs considered: 10 < epoch < 90\")\n",
    "    print(\"Mean weighted F1 score:\", avg_f1)\n",
    "    print(\"Standard deviation of weighted F1 score:\", std_f1)\n",
    "\n",
    "    return\n",
    "\n",
    "for key, file_path in file_path_dict.items():\n",
    "    print(f\"Metrics for {key}:\")\n",
    "    load_and_print_metrics(file_path)\n",
    "    print(\"\\n\")  # Add a newline for better readability between different metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2f297f14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics for original model:\n",
      "Range of epochs considered: 10 < epoch < 90\n",
      "Mean weighted F1 score: 0.6880503302025738\n",
      "Standard deviation of weighted F1 score: 0.018251090793725715 \n",
      "\n",
      "Metrics for FIRST finetune (warmup320):\n",
      "Range of epochs considered: 10 < epoch < 90\n",
      "Mean weighted F1 score: 0.7202618834250855\n",
      "Standard deviation of weighted F1 score: 0.007846184799475166 \n",
      "\n",
      "Metrics for SECOND finetune (warmup100):\n",
      "Range of epochs considered: 10 < epoch < 90\n",
      "Mean weighted F1 score: 0.7189347043670868\n",
      "Standard deviation of weighted F1 score: 0.005220081490266674 \n",
      "\n",
      "Metrics for only LLM finetune:\n",
      "Range of epochs considered: 10 < epoch < 90\n",
      "Mean weighted F1 score: 0.720088292119242\n",
      "Standard deviation of weighted F1 score: 0.008673478412130007 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "FIRST_paths = [\n",
    "    r\"/home/noxiusk/Desktop/data_science/dissertation/main/K-fold/large-cross-entropy--5e-5/fold1_FIRST_finetune/warmup320/metrics/roberta-large_metrics.csv\",\n",
    "    r\"/home/noxiusk/Desktop/data_science/dissertation/main/K-fold/large-cross-entropy--5e-5/fold2_FIRST_finetune/warmup320/metrics/roberta-large_metrics.csv\",\n",
    "    r\"/home/noxiusk/Desktop/data_science/dissertation/main/K-fold/large-cross-entropy--5e-5/fold3_FIRST_finetune/warmup320/metrics/roberta-large_metrics.csv\",\n",
    "    r\"/home/noxiusk/Desktop/data_science/dissertation/main/K-fold/large-cross-entropy--5e-5/fold4_FIRST_finetune/warmup320/metrics/roberta-large_metrics.csv\",\n",
    "    r\"/home/noxiusk/Desktop/data_science/dissertation/main/K-fold/large-cross-entropy--5e-5/fold5_FIRST_finetune/warmup320/metrics/roberta-large_metrics.csv\",\n",
    "]\n",
    "\n",
    "\n",
    "SECOND_paths = [\n",
    "    r\"/home/noxiusk/Desktop/data_science/dissertation/main/K-fold/large-cross-entropy--5e-5/fold3_FIRST_finetune/warmup320/fold3_second_finetune/warmup340/metrics/roberta-large_metrics.csv\",\n",
    "    r\"/home/noxiusk/Desktop/data_science/dissertation/main/K-fold/large-cross-entropy--5e-5/fold2_FIRST_finetune/warmup320/fold2_second_finetune/warmup340/metrics/roberta-large_metrics.csv\",\n",
    "    r\"/home/noxiusk/Desktop/data_science/dissertation/main/K-fold/large-cross-entropy--5e-5/fold4_FIRST_finetune/warmup320/fold4_second_finetune/warmup340/metrics/roberta-large_metrics.csv\",\n",
    "    r\"/home/noxiusk/Desktop/data_science/dissertation/main/K-fold/large-cross-entropy--5e-5/fold5_FIRST_finetune/warmup320/fold5_second_finetune/warmup340/metrics/roberta-large_metrics.csv\",\n",
    "    r\"/home/noxiusk/Desktop/data_science/dissertation/main/K-fold/large-cross-entropy--5e-5/fold1_FIRST_finetune/warmup320/fold1_SECOND_finetune/warmup340/metrics/roberta-large_metrics.csv\",\n",
    "]\n",
    "\n",
    "only_llm_paths = [\n",
    "    r\"/home/noxiusk/Desktop/data_science/dissertation/main/K-fold/large-cross-entropy--5e-5/fold1_FIRST_finetune/only_llm/metrics/roberta-large_metrics.csv\",\n",
    "    r\"/home/noxiusk/Desktop/data_science/dissertation/main/K-fold/large-cross-entropy--5e-5/fold2_FIRST_finetune/only_llm/metrics/roberta-large_metrics.csv\",\n",
    "    r\"/home/noxiusk/Desktop/data_science/dissertation/main/K-fold/large-cross-entropy--5e-5/fold3_FIRST_finetune/only_llm/metrics/roberta-large_metrics.csv\",\n",
    "    r\"/home/noxiusk/Desktop/data_science/dissertation/main/K-fold/large-cross-entropy--5e-5/fold4_FIRST_finetune/only_llm/metrics/roberta-large_metrics.csv\",\n",
    "    r\"/home/noxiusk/Desktop/data_science/dissertation/main/K-fold/large-cross-entropy--5e-5/fold5_FIRST_finetune/only_llm/metrics/roberta-large_metrics.csv\",\n",
    "]\n",
    "\n",
    "#load the dataframes\n",
    "dataframes_FIRST = [pd.read_csv(file_path) for file_path in FIRST_paths]\n",
    "dataframes_SECOND = [pd.read_csv(file_path) for file_path in SECOND_paths]\n",
    "dataframes_only_llm = [pd.read_csv(file_path) for file_path in only_llm_paths]\n",
    "\n",
    "#average results\n",
    "FIRST_avg = pd.concat(dataframes_FIRST).groupby(level=0).mean()\n",
    "SECOND_avg = pd.concat(dataframes_SECOND).groupby(level=0).mean()\n",
    "only_llm_avg = pd.concat(dataframes_only_llm).groupby(level=0).mean()\n",
    "original = pd.read_csv(r\"/home/noxiusk/Desktop/data_science/dissertation/main/K-fold/large-cross-entropy--5e-5/metrics/roberta-large_kfold_summary_metrics.csv\")\n",
    "\n",
    "# Calculate standard deviations and add them as new columns\n",
    "FIRST_std = pd.concat(dataframes_FIRST).groupby(level=0).std()\n",
    "SECOND_std = pd.concat(dataframes_SECOND).groupby(level=0).std()\n",
    "only_llm_std = pd.concat(dataframes_only_llm).groupby(level=0).std()\n",
    "\n",
    "# Add standard deviation columns to each dataframe\n",
    "for metric in ['eval_accuracy', 'eval_f1_weighted', 'eval_precision_weighted', 'eval_recall_weighted']:\n",
    "    FIRST_avg[f'std_{metric}'] = FIRST_std[metric]\n",
    "    SECOND_avg[f'std_{metric}'] = SECOND_std[metric]\n",
    "    only_llm_avg[f'std_{metric}'] = only_llm_std[metric]\n",
    "\n",
    "# delete from original the raws with matric_type != 'averaged_per_epoch'\n",
    "original = original[original['metric_type'] == 'averaged_per_epoch'].reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "def load_and_print_metrics(df):\n",
    "    # average adn std of weighted f1 score (epoch > 10 AND epoch < 20)\n",
    "    try:\n",
    "        avg_f1 = df.loc[(df['epoch'] > 10) & (df['epoch'] < 90), 'eval_f1_weighted'].mean()\n",
    "        std_f1 = df.loc[(df['epoch'] > 10) & (df['epoch'] < 90), 'eval_f1_weighted'].std()\n",
    "    except KeyError:\n",
    "        avg_f1 = df.loc[(df['epoch'] > 10) & (df['epoch'] < 90), 'avg_eval_f1_weighted'].mean()\n",
    "        std_f1 = df.loc[(df['epoch'] > 10) & (df['epoch'] < 90), 'avg_eval_f1_weighted'].std()\n",
    "\n",
    "    print(\"Range of epochs considered: 10 < epoch < 90\")\n",
    "    print(\"Mean weighted F1 score:\", avg_f1)\n",
    "    print(\"Standard deviation of weighted F1 score:\", std_f1, '\\n')\n",
    "\n",
    "print(\"\\nMetrics for original model:\")\n",
    "load_and_print_metrics(original)\n",
    "\n",
    "print(\"Metrics for FIRST finetune (warmup320):\")\n",
    "load_and_print_metrics(FIRST_avg)\n",
    "\n",
    "print(\"Metrics for SECOND finetune (warmup100):\")\n",
    "load_and_print_metrics(SECOND_avg)\n",
    "\n",
    "print(\"Metrics for only LLM finetune:\")\n",
    "load_and_print_metrics(only_llm_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2cbf8bf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best epoch for only LLM finetune:\n",
      "Best epoch: 20.0, \n",
      "Best F1 score: 3.317203 \\pm 73.582908, \n",
      "Best Accuracy: 74.000000 \\pm 3.162278\n",
      "\n",
      "Best epoch for FIRST finetune (warmup320):\n",
      "Best epoch: 22.995060633695637, \n",
      "Best F1 score: 5.556809 \\pm 73.805530, \n",
      "Best Accuracy: 74.400000 \\pm 5.176872\n",
      "\n",
      "Best epoch for SECOND finetune (warmup340):\n",
      "Best epoch: 21.9950184363007, \n",
      "Best F1 score: 5.788432 \\pm 72.838383, \n",
      "Best Accuracy: 73.600000 \\pm 5.412947\n",
      "\n",
      "Best epoch for original model:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:14: SyntaxWarning: invalid escape sequence '\\p'\n",
      "<>:14: SyntaxWarning: invalid escape sequence '\\p'\n",
      "<>:14: SyntaxWarning: invalid escape sequence '\\p'\n",
      "<>:14: SyntaxWarning: invalid escape sequence '\\p'\n",
      "/tmp/ipykernel_40169/500610779.py:14: SyntaxWarning: invalid escape sequence '\\p'\n",
      "  print(f\"Best epoch: {best_epoch}, \\nBest F1 score: {best_f1:1f} \\pm {best_f1_std:1f}, \\nBest Accuracy: {best_accuracy:1f} \\pm {best_accuracy_std:1f}\")\n",
      "/tmp/ipykernel_40169/500610779.py:14: SyntaxWarning: invalid escape sequence '\\p'\n",
      "  print(f\"Best epoch: {best_epoch}, \\nBest F1 score: {best_f1:1f} \\pm {best_f1_std:1f}, \\nBest Accuracy: {best_accuracy:1f} \\pm {best_accuracy_std:1f}\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>step_train</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>step_eval</th>\n",
       "      <th>eval_loss</th>\n",
       "      <th>eval_accuracy</th>\n",
       "      <th>eval_f1_weighted</th>\n",
       "      <th>eval_precision_weighted</th>\n",
       "      <th>eval_recall_weighted</th>\n",
       "      <th>std_eval_accuracy</th>\n",
       "      <th>std_eval_f1_weighted</th>\n",
       "      <th>std_eval_precision_weighted</th>\n",
       "      <th>std_eval_recall_weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>77.995061</td>\n",
       "      <td>6255.6</td>\n",
       "      <td>0.00158</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>6255.6</td>\n",
       "      <td>3.032843</td>\n",
       "      <td>0.732</td>\n",
       "      <td>0.726271</td>\n",
       "      <td>0.74818</td>\n",
       "      <td>0.732</td>\n",
       "      <td>0.051186</td>\n",
       "      <td>0.052523</td>\n",
       "      <td>0.06013</td>\n",
       "      <td>0.051186</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        epoch  step_train  train_loss  learning_rate  step_eval  eval_loss  \\\n",
       "77  77.995061      6255.6     0.00158       0.000011     6255.6   3.032843   \n",
       "\n",
       "    eval_accuracy  eval_f1_weighted  eval_precision_weighted  \\\n",
       "77          0.732          0.726271                  0.74818   \n",
       "\n",
       "    eval_recall_weighted  std_eval_accuracy  std_eval_f1_weighted  \\\n",
       "77                 0.732           0.051186              0.052523   \n",
       "\n",
       "    std_eval_precision_weighted  std_eval_recall_weighted  \n",
       "77                      0.06013                  0.051186  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#identify and print epoch with best f1 score of FIRST_avg and original\n",
    "def best_epoch(df):\n",
    "    try:\n",
    "        best_epoch = df.loc[df['eval_f1_weighted'].idxmax(), 'epoch']\n",
    "        best_f1 = df.loc[df['eval_f1_weighted'].idxmax(), 'std_eval_f1_weighted']*100\n",
    "        best_f1_std = df.loc[df['eval_f1_weighted'].idxmax(), 'eval_f1_weighted']*100\n",
    "\n",
    "        best_accuracy = df.loc[df['eval_f1_weighted'].idxmax(), 'eval_accuracy']*100\n",
    "        best_accuracy_std = df.loc[df['eval_f1_weighted'].idxmax(), 'std_eval_accuracy']*100\n",
    "    except KeyError:\n",
    "        best_epoch = df.loc[df['avg_eval_f1_weighted'].idxmax(), 'epoch']\n",
    "        best_f1 = df.loc[df['avg_eval_f1_weighted'].idxmax(), 'avg_eval_f1_weighted']\n",
    "        best_accuracy = df.loc[df['avg_eval_f1_weighted'].idxmax(), 'avg_eval_accuracy']\n",
    "    print(f\"Best epoch: {best_epoch}, \\nBest F1 score: {best_f1:1f} \\pm {best_f1_std:1f}, \\nBest Accuracy: {best_accuracy:1f} \\pm {best_accuracy_std:1f}\")\n",
    "\n",
    "print(\"\\nBest epoch for only LLM finetune:\")\n",
    "best_epoch(only_llm_avg)\n",
    "\n",
    "print(\"\\nBest epoch for FIRST finetune (warmup320):\")\n",
    "best_epoch(FIRST_avg)\n",
    "\n",
    "print(\"\\nBest epoch for SECOND finetune (warmup340):\")\n",
    "best_epoch(SECOND_avg)\n",
    "\n",
    "print(\"\\nBest epoch for original model:\")\n",
    "#best_epoch(original)\n",
    "\n",
    "FIRST_avg.iloc[[77]]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d20e94bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.53521126760563"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "100*(73.6-71.8)/71.0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
