{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217890ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:93: SyntaxWarning: invalid escape sequence '\\p'\n",
      "<>:94: SyntaxWarning: invalid escape sequence '\\p'\n",
      "<>:95: SyntaxWarning: invalid escape sequence '\\p'\n",
      "<>:93: SyntaxWarning: invalid escape sequence '\\p'\n",
      "<>:94: SyntaxWarning: invalid escape sequence '\\p'\n",
      "<>:95: SyntaxWarning: invalid escape sequence '\\p'\n",
      "/tmp/ipykernel_37520/583358537.py:93: SyntaxWarning: invalid escape sequence '\\p'\n",
      "  print(f\"Accuracy: {accuracy:.1f} \\pm {accuracy_std:.1f}\")\n",
      "/tmp/ipykernel_37520/583358537.py:94: SyntaxWarning: invalid escape sequence '\\p'\n",
      "  print(f\"Weighted F1-score: {weighted_f1:.1f} \\pm {weighted_f1_std:.1f}\")\n",
      "/tmp/ipykernel_37520/583358537.py:95: SyntaxWarning: invalid escape sequence '\\p'\n",
      "  print(f\"Macro-averaged F1-score: {macro_f1:.1f} \\pm {macro_f1_std:.1f}\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.utils import resample\n",
    "\n",
    "def evaluate(file_path):\n",
    "    \"\"\"\n",
    "    Evaluates the performance of a model based on true and predicted labels from a CSV file.\n",
    "    \n",
    "    Parameters:\n",
    "    - file_path: str, path to the CSV file containing 'true_label_str' and 'predicted_label' columns.\n",
    "    \n",
    "    Returns:\n",
    "    - Prints accuracy, F1 score, recall, and precision metrics with standard deviations.\n",
    "    \"\"\"\n",
    "    # Load the CSV file\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file '{file_path}' was not found.\")\n",
    "        exit()\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading CSV file: {e}\")\n",
    "        exit()\n",
    "\n",
    "    # Extract true and predicted labels\n",
    "    # Convert to lowercase to handle inconsistencies like 'Behavior' vs 'behavior'\n",
    "    try:\n",
    "        true_labels = df['true_label_str'].astype(str).str.lower()\n",
    "        predicted_labels = df['predicted_label'].astype(str).str.lower()\n",
    "    except KeyError as e:\n",
    "        print(f\"Error: Column {e} not found in the CSV. Please check column names.\")\n",
    "        exit()\n",
    "\n",
    "    # Number of rows\n",
    "    num_rows = len(df)\n",
    "\n",
    "    # Ensure there are labels to process\n",
    "    if num_rows == 0:\n",
    "        print(\"The CSV file is empty or has no data rows.\")\n",
    "        exit()\n",
    "\n",
    "    # --- Metric Calculations ---\n",
    "    le = LabelEncoder()\n",
    "    all_labels = pd.concat([true_labels, predicted_labels]).unique()\n",
    "    le.fit(all_labels)\n",
    "\n",
    "    true_labels_encoded = le.transform(true_labels)\n",
    "    predicted_labels_encoded = le.transform(predicted_labels)\n",
    "\n",
    "    # Get the unique labels present in the true labels\n",
    "    unique_true_labels_encoded = pd.Series(true_labels_encoded).unique()\n",
    "\n",
    "    # Calculate metrics on full dataset\n",
    "    accuracy = accuracy_score(true_labels_encoded, predicted_labels_encoded)*100\n",
    "    weighted_f1 = f1_score(true_labels_encoded, predicted_labels_encoded, average='weighted', zero_division=0)*100\n",
    "    macro_f1 = f1_score(true_labels_encoded, predicted_labels_encoded, average='macro', labels=unique_true_labels_encoded, zero_division=0)*100\n",
    "\n",
    "    # Calculate standard deviations using bootstrap resampling\n",
    "    n_bootstrap = 1000\n",
    "    random_state = 42\n",
    "    \n",
    "    accuracies = []\n",
    "    weighted_f1s = []\n",
    "    macro_f1s = []\n",
    "    \n",
    "    # Create a DataFrame with true and predicted labels for easy resampling\n",
    "    bootstrap_data = pd.DataFrame({'true': true_labels_encoded, 'pred': predicted_labels_encoded})\n",
    "    \n",
    "    for i in range(n_bootstrap):\n",
    "        # Resample with replacement\n",
    "        boot_sample = resample(bootstrap_data, replace=True, n_samples=len(bootstrap_data), random_state=random_state+i)\n",
    "        \n",
    "        boot_true = boot_sample['true'].values\n",
    "        boot_pred = boot_sample['pred'].values\n",
    "        \n",
    "        boot_unique_true_labels = pd.Series(boot_true).unique()\n",
    "        \n",
    "        accuracies.append(accuracy_score(boot_true, boot_pred))\n",
    "        weighted_f1s.append(f1_score(boot_true, boot_pred, average='weighted', zero_division=0))\n",
    "        macro_f1s.append(f1_score(boot_true, boot_pred, average='macro', labels=boot_unique_true_labels, zero_division=0))\n",
    "    \n",
    "    accuracy_std = np.std(accuracies)*100\n",
    "    weighted_f1_std = np.std(weighted_f1s)*100\n",
    "    macro_f1_std = np.std(macro_f1s)*100\n",
    "\n",
    "    # Print the results\n",
    "    print(f\"--- Metrics for {file_path} ---\")\n",
    "    print(f\"Number of rows: {num_rows}\")\n",
    "    print(f\"Agreement: {(num_rows/500)*100:.1f}%\")\n",
    "    print(f\"Accuracy: {accuracy:.1f} \\pm {accuracy_std:.1f}\")\n",
    "    print(f\"Weighted F1-score: {weighted_f1:.1f} \\pm {weighted_f1_std:.1f}\")\n",
    "    print(f\"Macro-averaged F1-score: {macro_f1:.1f} \\pm {macro_f1_std:.1f}\")\n",
    "\n",
    "    # Print class proportions\n",
    "    class_proportions = df['true_label_str'].value_counts(normalize=True)\n",
    "    print(\"\\nClass Proportions: \", class_proportions)\n",
    "    print(\"\\nClass Proportions:\")\n",
    "    for label, proportion in class_proportions.items():\n",
    "        print(f\"{label}: {proportion:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4bf7ecdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Metrics for train_comb_deepT+grokN.csv ---\n",
      "Number of rows: 398\n",
      "Agreement: 79.6%\n",
      "Accuracy: 84.2 \\pm 1.8\n",
      "Weighted F1-score: 84.1 \\pm 1.8\n",
      "Macro-averaged F1-score: 83.5 \\pm 2.1\n",
      "\n",
      "Class Proportions:  true_label_str\n",
      "ideation     0.419598\n",
      "behavior     0.271357\n",
      "indicator    0.228643\n",
      "attempt      0.080402\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Class Proportions:\n",
      "ideation: 0.4196\n",
      "behavior: 0.2714\n",
      "indicator: 0.2286\n",
      "attempt: 0.0804\n"
     ]
    }
   ],
   "source": [
    "file_path = r'train_comb_deepT+grokN.csv'\n",
    "evaluate(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c32e834b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Metrics for train_comb_deepT+grokT+grokN.csv ---\n",
      "Number of rows: 373\n",
      "Agreement: 74.6%\n",
      "Accuracy: 86.6 \\pm 1.8\n",
      "Weighted F1-score: 86.5 \\pm 1.8\n",
      "Macro-averaged F1-score: 85.7 \\pm 2.0\n",
      "\n",
      "Class Proportions:  true_label_str\n",
      "ideation     0.426273\n",
      "behavior     0.265416\n",
      "indicator    0.227882\n",
      "attempt      0.080429\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Class Proportions:\n",
      "ideation: 0.4263\n",
      "behavior: 0.2654\n",
      "indicator: 0.2279\n",
      "attempt: 0.0804\n"
     ]
    }
   ],
   "source": [
    "file_path = r'train_comb_deepT+grokT+grokN.csv'\n",
    "evaluate(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5fdadae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Metrics for train_comb_deepT+geminiN+grokT+grokN.csv ---\n",
      "Number of rows: 342\n",
      "Agreement: 68.4%\n",
      "Accuracy: 88.3 \\pm 1.7\n",
      "Weighted F1-score: 88.2 \\pm 1.7\n",
      "Macro-averaged F1-score: 86.8 \\pm 2.2\n",
      "\n",
      "Class Proportions:  true_label_str\n",
      "ideation     0.435673\n",
      "behavior     0.269006\n",
      "indicator    0.210526\n",
      "attempt      0.084795\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Class Proportions:\n",
      "ideation: 0.4357\n",
      "behavior: 0.2690\n",
      "indicator: 0.2105\n",
      "attempt: 0.0848\n"
     ]
    }
   ],
   "source": [
    "file_path = r'train_comb_deepT+geminiN+grokT+grokN.csv'\n",
    "evaluate(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085dc31d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: The file 'f:\\VERO UTENTE\\Desktop\\Uni\\dissertation\\main\\NO-fold\\validation_split_comb_fold1+deepT+geminiN+grokT+grokN.csv' was not found.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34889/583358537.py:93: SyntaxWarning: invalid escape sequence '\\p'\n",
      "  print(f\"Accuracy: {accuracy:.1f} \\pm {accuracy_std:.1f}\")\n",
      "/tmp/ipykernel_34889/583358537.py:94: SyntaxWarning: invalid escape sequence '\\p'\n",
      "  print(f\"Weighted F1-score: {weighted_f1:.1f} \\pm {weighted_f1_std:.1f}\")\n",
      "/tmp/ipykernel_34889/583358537.py:95: SyntaxWarning: invalid escape sequence '\\p'\n",
      "  print(f\"Macro-averaged F1-score: {macro_f1:.1f} \\pm {macro_f1_std:.1f}\")\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'df' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 11\u001b[0m\n\u001b[1;32m      1\u001b[0m file_paths \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mVERO UTENTE\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDesktop\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUni\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mdissertation\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mmain\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mNO-fold\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mvalidation_split_comb_fold1+deepT+geminiN+grokT+grokN.csv\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mVERO UTENTE\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDesktop\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUni\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mdissertation\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mmain\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mNO-fold\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mvalidation_split_comb_fold2+deepT+geminiN+grokT+grokN.csv\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mVERO UTENTE\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDesktop\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUni\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mdissertation\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mmain\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mNO-fold\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mvalidation_split_comb_fold5+deepT+geminiN+grokT+grokN.csv\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      7\u001b[0m ]\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_path \u001b[38;5;129;01min\u001b[39;00m file_paths:\n\u001b[0;32m---> 11\u001b[0m     evaluate(file_path)\n",
      "Cell \u001b[0;32mIn[1], line 31\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Extract true and predicted labels\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Convert to lowercase to handle inconsistencies like 'Behavior' vs 'behavior'\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 31\u001b[0m     true_labels \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrue_label_str\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mlower()\n\u001b[1;32m     32\u001b[0m     predicted_labels \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredicted_label\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mlower()\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: cannot access local variable 'df' where it is not associated with a value"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "file_paths = [\n",
    "    r'/home/noxiusk/Desktop/data_science/dissertation/main/NO-fold/validation_split_comb_fold1+deepT+geminiN+grokT+grokN.csv',\n",
    "    r'/home/noxiusk/Desktop/data_science/dissertation/main/NO-fold/validation_split_comb_fold2+deepT+geminiN+grokT+grokN.csv',\n",
    "    r'/home/noxiusk/Desktop/data_science/dissertation/main/NO-fold/validation_split_comb_fold3+deepT+geminiN+grokT+grokN.csv',\n",
    "    r'/home/noxiusk/Desktop/data_science/dissertation/main/NO-fold/validation_split_comb_fold4+deepT+geminiN+grokT+grokN.csv',\n",
    "    r'/home/noxiusk/Desktop/data_science/dissertation/main/NO-fold/validation_split_comb_fold5+deepT+geminiN+grokT+grokN.csv',\n",
    "]\n",
    "\n",
    "for file_path in file_paths:\n",
    "    evaluate(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c266e613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Indicator: 0.2215\n",
      "Average Ideation: 0.4681\n",
      "Average Behavior: 0.2165\n",
      "Average Attempt: 0.0718\n"
     ]
    }
   ],
   "source": [
    "indicator = [0.1754, 0.2273, 0.2295, 0.2295, 0.2459]\n",
    "ideation = [0.4561,  0.4091, 0.5246, 0.4918, 0.4590]\n",
    "behavior = [0.2632, 0.2576, 0.1803, 0.2295, 0.2623, 0.1061]\n",
    "attempt = [0.0328, 0.0492, 0.0656, 0.1061, 0.1053]\n",
    "\n",
    "avg_indicator = sum(indicator) / len(indicator)\n",
    "avg_ideation = sum(ideation) / len(ideation)\n",
    "avg_behavior = sum(behavior) / len(behavior)\n",
    "avg_attempt = sum(attempt) / len(attempt)\n",
    "\n",
    "print(f\"\\nAverage Indicator: {avg_indicator:.4f}\")\n",
    "print(f\"Average Ideation: {avg_ideation:.4f}\")\n",
    "print(f\"Average Behavior: {avg_behavior:.4f}\")\n",
    "print(f\"Average Attempt: {avg_attempt:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200a6ed0",
   "metadata": {},
   "source": [
    "AVG RESULTS:\n",
    "\n",
    "Number of rows: 61\n",
    "accuracy: 0.8931\n",
    "Weighted F1: 0.8891\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d44c25",
   "metadata": {},
   "source": [
    "*****TEST CHECK*****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a803d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: The file 'f:\\VERO UTENTE\\Desktop\\Uni\\dissertation\\main\\NO-fold\\validation_split_comb_fold1+deepT+geminiN+grokT+grokN.csv' was not found.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_37520/583358537.py:93: SyntaxWarning: invalid escape sequence '\\p'\n",
      "  print(f\"Accuracy: {accuracy:.1f} \\pm {accuracy_std:.1f}\")\n",
      "/tmp/ipykernel_37520/583358537.py:94: SyntaxWarning: invalid escape sequence '\\p'\n",
      "  print(f\"Weighted F1-score: {weighted_f1:.1f} \\pm {weighted_f1_std:.1f}\")\n",
      "/tmp/ipykernel_37520/583358537.py:95: SyntaxWarning: invalid escape sequence '\\p'\n",
      "  print(f\"Macro-averaged F1-score: {macro_f1:.1f} \\pm {macro_f1_std:.1f}\")\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'df' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#deep-seek\u001b[39;00m\n\u001b[1;32m      2\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mVERO UTENTE\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDesktop\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUni\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mdissertation\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mmain\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mNO-fold\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mvalidation_split_comb_fold1+deepT+geminiN+grokT+grokN.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 3\u001b[0m evaluate(file_path)\n",
      "Cell \u001b[0;32mIn[1], line 31\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Extract true and predicted labels\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Convert to lowercase to handle inconsistencies like 'Behavior' vs 'behavior'\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 31\u001b[0m     true_labels \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrue_label_str\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mlower()\n\u001b[1;32m     32\u001b[0m     predicted_labels \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredicted_label\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mlower()\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: cannot access local variable 'df' where it is not associated with a value"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#deep-seek\n",
    "file_path = r'f:\\VERO UTENTE\\Desktop\\Uni\\dissertation\\main\\NO-fold\\validation_split_comb_fold1+deepT+geminiN+grokT+grokN.csv'\n",
    "evaluate(file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
